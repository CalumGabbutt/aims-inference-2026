{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39dfa70",
   "metadata": {},
   "source": [
    "# Tutorial on model selection/sparsification \n",
    "using 1D polynomial regression as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a75295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b09f5",
   "metadata": {},
   "source": [
    "# Generate Data\n",
    "We shall do a 1D polynomial regression problem: assume $y = f(x,\\mathbf{p}) = \\Theta(x) \\cdot \\mathbf{p}$, where $\\Theta(x)$ is a library of polynomials of $x$, we want to infer $p$ from some observations of ${\\hat{x},\\hat{y}}$ pairs. Assume $\\hat{y}$ is contaminated by some Gaussian noise of known variance $\\sigma$.\n",
    "\n",
    "First, let's generate some data.\n",
    "\n",
    "1. Select some sampling points in $x$\n",
    "2. Create an arbitrary polynomial function with sparse terms\n",
    "3. Add noise to $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = -0.1*x**7  + 0.5*x**4 + 0.5 * x**3 - 3.0 * x**2 #+ 2.0 * x + 1.0 # Some random polynomial function - Play with it!\n",
    "sigma = 0.5 # Standard deviation of noise - Play with it!\n",
    "yData = y + sigma * np.random.normal(size=x.shape)\n",
    "\n",
    "# Wider sample point (for checking extrapolation)\n",
    "x_wide = np.linspace(-10, 10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ad21e",
   "metadata": {},
   "source": [
    "Let's visualise what we have generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Sample Data with Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8609ff",
   "metadata": {},
   "source": [
    "# Create a polynomial library\n",
    "To prepare for the regression, let's create some helper function to evaluate the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27efc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_library(x, degree):\n",
    "    \"\"\"\n",
    "    Generate a library of polynomial features up to the given degree for input x.\n",
    "    Returns a matrix where each column is x**i for i in [0, degree].\n",
    "    \"\"\"\n",
    "    return np.vstack([x**i for i in range(degree + 1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features up to degree 7 - output: N x (degree+1) matrix\n",
    "Theta = polynomial_library(x,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5841e3",
   "metadata": {},
   "source": [
    "# Ordinary Least Square regression\n",
    "The easiest way to do regression is, of course, by linear least square.\n",
    "\n",
    "Let's try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d013ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression to get coefficients p (OLS)\n",
    "p_OLS = np.linalg.inv(Theta.T @ Theta) @ Theta.T @ yData\n",
    "# Alternatively, use the scikit-learn library\n",
    "# p_OLS = linear_model.LinearRegression().fit(Theta, yData).coef_\n",
    "# Predicted output using OLS coefficients\n",
    "y_OLS = Theta @ p_OLS\n",
    "\n",
    "print(\"OLS Coefficients:\", p_OLS) # Are the coefficients close to the true polynomial coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2819e",
   "metadata": {},
   "source": [
    "Let's visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.plot(x, y_OLS, label='OLS', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb7b1c",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "\n",
    "1. Does it fit well? \n",
    "2. What if we increase $\\sigma$? \n",
    "3. What if we decrease the number of data points?\n",
    "4. What if we increase the degree in the polynomial library?\n",
    "   - How should we determine the library size?\n",
    "5. What if we extrapolate to outside the sampled range?\n",
    "\n",
    "What is the key to a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa24d7",
   "metadata": {},
   "source": [
    "# Regularised Regression (Ridge,LASSO)\n",
    "Now, let's try to redo the above with some regularisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "# Regression to get coefficients p (Ridge)\n",
    "# p_Ridge = np.linalg.solve(Theta.T @ Theta + alpha * sigma**2 * np.eye(Theta.shape[1]), Theta.T @ yData)\n",
    "# Regression to get coefficients p (Ridge)\n",
    "p_Ridge = linear_model.Ridge(alpha=(alpha * sigma**2), fit_intercept=False).fit(Theta, yData).coef_\n",
    "# Predicted output using Ridge coefficients\n",
    "y_Ridge = Theta @ p_Ridge\n",
    "\n",
    "print(\"Ridge Coefficients:\", p_Ridge) # Are the coefficients close to the true polynomial coefficients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_LASSO = 1.0\n",
    "# Regression to get coefficients p (Lasso)\n",
    "p_Lasso = linear_model.Lasso(alpha=(alpha_LASSO * sigma**2), fit_intercept=False).fit(Theta, yData).coef_\n",
    "# Predicted output using Lasso coefficients\n",
    "y_Lasso = Theta @ p_Lasso\n",
    "\n",
    "print(\"Lasso Coefficients:\", p_Lasso) # Are the coefficients close to the true polynomial coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d602a",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "\n",
    "1. Which regularisation give more zero coefficients (sparsification)?\n",
    "2. Which one gives better prediction?\n",
    "   - Which one generalise better outside the sampled domain?\n",
    "3. Which one is more noise robust (try change the noise)?\n",
    "4. How should we determine the value of $\\alpha$ in both regularisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cedb9",
   "metadata": {},
   "source": [
    "# Maximising Evidence by Greedy search\n",
    "In this section, we will employ the idea of maximising evidence to select the right term in the library. The idea is to start from the full library, and remove terms one by one according to which removal maximise the evidence.\n",
    "\n",
    "To create this algorithm, we will need to\n",
    "1. Implement a function that calculate the evidence given the active terms\n",
    "2. Implement a Greedy search loop\n",
    "\n",
    "We will assume ridge regression in this algorithm (but feel free to try other regulariser too!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the regression and the evidence\n",
    "def evidence(Theta,target, alpha, sigma2):\n",
    "    \"\"\"\n",
    "    Compute the log-evidence of the model given design matrix Theta, target data,\n",
    "    regularization parameter alpha, and noise standard deviation sigma.\n",
    "    \"\"\"\n",
    "    A = (Theta.T @ Theta)/sigma2 + np.eye(Theta.shape[1])*alpha\n",
    "    coef = np.linalg.solve(A, (Theta.T @ target)/sigma2)\n",
    "    \n",
    "    JMap = 0.5 * (np.sum((target - Theta @ coef)**2) / sigma2 + alpha * np.sum(coef**2)) \\\n",
    "        + 0.5 * np.log(2*np.pi*sigma2) * target.shape[0] + 0.5 * np.log(2*np.pi*alpha) * Theta.shape[1]\n",
    "    return coef, JMap + 0.5 * np.linalg.slogdet(A/2/np.pi)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eaf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(Theta.shape[1], dtype=bool)\n",
    "coef = np.zeros((Theta.shape[1],Theta.shape[1]))\n",
    "evidence_list = np.zeros(Theta.shape[1])\n",
    "alpha = 0.1\n",
    "# Greedy Search\n",
    "for i in range(Theta.shape[1]):\n",
    "    best_evidence = np.inf\n",
    "    best_idx = -1\n",
    "    best_coef_ = None\n",
    "    for j in range(Theta.shape[1]):\n",
    "        if not mask[j]:\n",
    "            continue\n",
    "        current_mask = mask.copy()\n",
    "        current_mask[j] = False\n",
    "        coef_, current_evidence = evidence(Theta[:, current_mask], yData, alpha, sigma**2)\n",
    "        if current_evidence < best_evidence:\n",
    "            best_evidence = current_evidence\n",
    "            best_coef_ = coef_\n",
    "            best_idx = j\n",
    "    if best_idx == -1:\n",
    "        break\n",
    "    mask[best_idx] = False\n",
    "    coef[i,mask] = best_coef_\n",
    "    evidence_list[i] = best_evidence\n",
    "\n",
    "    print(f\"Removed feature {best_idx}, New Evidence: {best_evidence}\")\n",
    "\n",
    "# Final selected model\n",
    "best_model_idx = np.argmin(evidence_list)\n",
    "p_Evidence = coef[best_model_idx,:]\n",
    "y_Evidence = Theta @ p_Evidence\n",
    "print(\"\\n Best model coeff:\", p_Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.plot(x, y_Evidence, label='Evidence', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68823fe2",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "\n",
    "1. How would you determine $\\alpha$?\n",
    "2. What happens when we play with $\\sigma$?\n",
    "3. What would you do if you do not know $\\sigma$ a priori?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bd9b6",
   "metadata": {},
   "source": [
    "# (optional) SparseBayes / ARD algorithm\n",
    "SparseBayes (a.k.a. ARDRegression) is an automatic algorithm that optimise the prior variance and sparsify the model by maiximising the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6782c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression to get coefficients p (Ridge)\n",
    "p_ARD = linear_model.ARDRegression().fit(Theta, yData).coef_\n",
    "# Predicted output using Ridge coefficients\n",
    "y_ARD = Theta @ p_ARD\n",
    "\n",
    "print(\"ARD Coefficients:\", p_ARD) # Are the coefficients close to the true polynomial coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.plot(x, y_ARD, label='ARD', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f3b89",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "How does it compare with your implementation of the Bayesian Evidence Maximisation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
