{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39dfa70",
   "metadata": {},
   "source": [
    "# Tutorial on model selection/sparsification \n",
    "using 1D polynomial regression as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a75295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b09f5",
   "metadata": {},
   "source": [
    "# Generate Data\n",
    "We shall do a 1D polynomial regression problem: assume $y = f(x,\\mathbf{p}) = \\Theta(x) \\cdot \\mathbf{p}$, where $\\Theta(x)$ is a library of polynomials of $x$, we want to infer $p$ from some observations of ${\\hat{x},\\hat{y}}$ pairs. Assume $\\hat{y}$ is contaminated by some Gaussian noise of known variance $\\sigma$.\n",
    "\n",
    "First, let's generate some data.\n",
    "\n",
    "1. Select some sampling points in $x$\n",
    "2. Create an arbitrary polynomial function with sparse terms\n",
    "3. Add noise to $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "x = np.linspace(-1, 1, 100)\n",
    "y = #(fill in) # Some random polynomial function of x\n",
    "sigma = 0.5 # Standard deviation of noise - Play with it!\n",
    "yData = y + sigma * np.random.normal(size=x.shape)\n",
    "\n",
    "# Wider sample point (for checking extrapolation)\n",
    "x_wide = #(fill in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ad21e",
   "metadata": {},
   "source": [
    "Let's visualise what we have generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Sample Data with Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8609ff",
   "metadata": {},
   "source": [
    "# Create a polynomial library\n",
    "To prepare for the regression, let's create some helper function to evaluate the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27efc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_library(x, degree):\n",
    "    \"\"\"\n",
    "    Generate a library of polynomial features up to the given degree for input x.\n",
    "    Returns a matrix where each column is x**i for i in [0, degree].\n",
    "    \"\"\"\n",
    "    return #(fill in) # Fill in to return the polynomial feature matrix [ 1 x x**2 x**3 ... x**degree ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ed29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features up to degree 7 - output: N x (degree+1) matrix\n",
    "Theta = polynomial_library(x,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5841e3",
   "metadata": {},
   "source": [
    "# Ordinary Least Square regression\n",
    "The easiest way to do regression is, of course, by linear least square.\n",
    "\n",
    "Let's try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d013ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression to get coefficients p (OLS)\n",
    "p_OLS = #(fill in)\n",
    "# Predicted output using OLS coefficients\n",
    "y_OLS = Theta @ p_OLS\n",
    "\n",
    "print(\"OLS Coefficients:\", p_OLS) # Are the coefficients close to the true polynomial coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e2819e",
   "metadata": {},
   "source": [
    "Let's visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.plot(x, y_OLS, label='OLS', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb7b1c",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "\n",
    "1. Does it fit well? \n",
    "2. What if we increase $\\sigma$? \n",
    "3. What if we decrease the number of data points?\n",
    "4. What if we increase the degree in the polynomial library?\n",
    "   - How should we determine the library size?\n",
    "5. What if we extrapolate to outside the sampled range?\n",
    "\n",
    "What is the key to a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa24d7",
   "metadata": {},
   "source": [
    "# Regularised Regression (Ridge,LASSO)\n",
    "Now, let's try to redo the above with some regularisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df7a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = #(fill in) # Regularization strength for Ridge regression\n",
    "# Regression to get coefficients p (Ridge)\n",
    "p_Ridge = #(fill in)\n",
    "# Predicted output using Ridge coefficients\n",
    "y_Ridge = Theta @ p_Ridge\n",
    "\n",
    "print(\"Ridge Coefficients:\", p_Ridge) # Are the coefficients close to the true polynomial coefficients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_LASSO = #(fill in) # Regularization strength for LASSO regression\n",
    "# Regression to get coefficients p (Lasso)\n",
    "p_Lasso = linear_model.Lasso(alpha=(alpha_LASSO * sigma**2), fit_intercept=False).fit(Theta, yData).coef_\n",
    "# Predicted output using Lasso coefficients\n",
    "y_Lasso = Theta @ p_Lasso\n",
    "\n",
    "print(\"Lasso Coefficients:\", p_Lasso) # Are the coefficients close to the true polynomial coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1d602a",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "\n",
    "1. Which regularisation give more zero coefficients (sparsification)?\n",
    "2. Which one gives better prediction?\n",
    "   - Which one generalise better outside the sampled domain?\n",
    "3. Which one is more noise robust (try change the noise)?\n",
    "4. How should we determine the value of $\\alpha$ in both regularisation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0cedb9",
   "metadata": {},
   "source": [
    "# Maximising Evidence by Greedy search\n",
    "In this section, we will employ the idea of maximising evidence to select the right term in the library. The idea is to start from the full library, and remove terms one by one according to which removal maximise the evidence.\n",
    "\n",
    "To create this algorithm, we will need to\n",
    "1. Implement a function that calculate the evidence given the active terms\n",
    "2. Implement a Greedy search loop\n",
    "\n",
    "We will assume ridge regression in this algorithm (but feel free to try other regulariser too!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecbb4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the regression and the evidence\n",
    "def evidence(Theta,target, alpha, sigma2):\n",
    "    \"\"\"\n",
    "    Compute the log-evidence of the model given design matrix Theta, target data,\n",
    "    regularization parameter alpha, and noise standard deviation sigma.\n",
    "    \"\"\"\n",
    "    A = #(fill in) # Matrix to be inverted to compute ridge regression\n",
    "    coef = np.linalg.solve(A, (Theta.T @ target)/sigma2)\n",
    "    \n",
    "    JMap = #(fill in) # Compute the negative log-posterior at the MAP estimate\n",
    "    JEvidence = JMap + #(fill in) Log(det(A/2/pi)) # Compute the negative log-evidence\n",
    "    return coef, JEvidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eaf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(Theta.shape[1], dtype=bool)\n",
    "coef = np.zeros((Theta.shape[1],Theta.shape[1]))\n",
    "evidence_list = np.zeros(Theta.shape[1])\n",
    "alpha = #(fill in) # Regularization strength for Evidence regression\n",
    "\n",
    "# Greedy Search - Search for the best combination of polynomial terms that maximizes the evidence\n",
    "for i in range(Theta.shape[1]):\n",
    "    #(fill in)\n",
    "    for j in range(Theta.shape[1]):\n",
    "        #(fill in)\n",
    "\n",
    "    #(fill in)\n",
    "\n",
    "# Final selected model\n",
    "#(fill in)\n",
    "print(\"\\n Best model coeff:\", p_Evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf0b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.plot(x, y_Evidence, label='Evidence', color='green')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68823fe2",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "\n",
    "1. How would you determine $\\alpha$?\n",
    "2. What happens when we play with $\\sigma$?\n",
    "3. What would you do if you do not know $\\sigma$ a priori?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bd9b6",
   "metadata": {},
   "source": [
    "# (optional) SparseBayes / ARD algorithm\n",
    "SparseBayes (a.k.a. ARDRegression) is an automatic algorithm that optimise the prior variance and sparsify the model by maiximising the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6782c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression to get coefficients p (Ridge)\n",
    "p_ARD = linear_model.ARDRegression().fit(Theta, yData).coef_\n",
    "# Predicted output using Ridge coefficients\n",
    "y_ARD = Theta @ p_ARD\n",
    "\n",
    "print(\"ARD Coefficients:\", p_ARD) # Are the coefficients close to the true polynomial coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bc9066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the result\n",
    "plt.scatter(x, yData, label='Data Points', color='blue', s=10)\n",
    "plt.plot(x, y, label='True Function', color='black')\n",
    "plt.plot(x, y_ARD, label='ARD', color='red')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5f3b89",
   "metadata": {},
   "source": [
    "### Food for thoughts\n",
    "How does it compare with your implementation of the Bayesian Evidence Maximisation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
